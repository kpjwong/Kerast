Keras notes:


Q: What are epochs, iteration, batch size?

A: These terms are used when SGD is used. i.e. data is too big to pass to the computer
   
One epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.
- More epochs are better because there will be more updates
- However, using too many epochs leads to overfitting
- Heuristically, the number of epochs depends on how diverse the dataset is

Batch size is the total number of training examples present in a single batch

Iteration is the number of batches needed to complete one epoch (i.e. N / batch_size)


Note: General format for the models under Sequential class

0: Import the essentials
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import SGD

1: Make sure training and testing data are of the right shapes and types
x_train = x_train.reshape(N,F)
x_train = x_train.astype('float32')
y_train = keras.utils.tp_categorical(y_train, M) # M being the number of classes

2: Setup model
model = Sequential()
model.add(Dense(n1, activation='sigmoid', input_shape = (F,)) # add first layer, need to specify input shape
model.add(Dense(n2, activation='sigmoid')) # no need to add input shape for layers following the first one
...
model.add(Dense(M, activation='softmax')) 

model.summary() # check model structure

3: Train and predict model
# Define optimizer and loss function
model.compile(loss='categorical_crossentropy',
              optimizer=SGD(),
metrics=['accuracy'])

# Train
history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_data=(x_test, y_test))

# Test
score = model.evaluate(x_test, y_test, verbose=0)



Some notes on glob, re, split

glob: finds all the pathnames matching a specified pattern according to the rules used by the Unix shell
      e.g. glob.glob(scrape_dir + "/*.fasta") #finds all fasta files in scrape_dir

re:   finds string patterns using regex syntax
      e.g. match = re.search(r'(regex expression)', s) #finds matches inside s, outputs as a tuple

str.split(): separate characters in str denominated by ' '


What lecture 13 is trying to do:

1. Parse protein squence data in the fasta file in the form: "ID(i): seq(i)\n", put in protein-sequence.txt
2. Search the catalogue and find all proteins that "has function ATP binding" (0005524)


Lecture 14:

Q: What to do when data points are of different lengths?
A: Pad them using the pad_sequence() method. Usually, we fill the initial space with respect to the max_length we set, with zero values.


Lecture 17:

Emedding layer:
- can only be first layer
- usually used when the data is in the form of large, sparse one-hot (categorical) vector. e.g. dictionary
- given a data in F one-hot vectors, an embedding layer transforms each of the F categories in terms of a K-vector, representing the       weights spanned by K latent variables
- syntax: keras.layers.Embedding(input_dim = F, output_dim = K, ..., input_lendth = N)

Flatten layer
- flattening is needed when the input tensor is D-dimensional with d > 2. By default, the first dimension has to be the data size
- similar to reshape, but: 1. applies on layers instead of arrays, 2. output dimensions need not be consistent with the total number of   elements.
- syntax: keras.layers.Flatten(data_format=None)


